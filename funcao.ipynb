{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função - Classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está função será chamada pelo botão \"Processar\" da inteface.\n",
    "\n",
    "Ela irá treinar um modelo com nossa base já classificada. Posteriormente irá pegar o excel selecioando e irá classificá-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de preparo as manhcetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegar só as manchetes e limpar \n",
    "# igual está no DATASET_PREPARATION!!!\n",
    "dataset = pd.read_excel(\"base_pronta.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por enquanto irei fazer considerando apenas o modelo SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    import string\n",
    "    punctuation = '[\\/!-.:?;]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepara_novo_data_set(dataset):\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    manchetes = {}\n",
    "    contador = 0\n",
    "    email=[]\n",
    "    lista_manchetes=[]\n",
    "    numero_atual = 0\n",
    "    loop = tqdm(total = data.shape[0], position = 0, leave = False)\n",
    "    for i in data.HTML.index:\n",
    "        loop.set_description(\"Extraindo manchetes...\".format(numero_atual))\n",
    "        loop.update(1)\n",
    "        numero_atual += 1\n",
    "        texto = str(data.HTML[i])\n",
    "\n",
    "        for indice in range(len(texto)):\n",
    "\n",
    "            if texto[indice:indice+7] == \"message\":\n",
    "\n",
    "                manchete_incial = texto[indice+11:indice+150]\n",
    "                achou = False\n",
    "                manchete = \" \"\n",
    "                indice_letra = 0\n",
    "                email.append(i+2)\n",
    "\n",
    "                while indice_letra < len(manchete_incial):\n",
    "\n",
    "                    if manchete_incial[indice_letra] == \"}\":\n",
    "\n",
    "                        manchete = manchete_incial[0:indice_letra-6]\n",
    "                        achou = True\n",
    "                        manchetes[contador] = manchete\n",
    "\n",
    "                        manchete_limpa = cleanup(manchete)\n",
    "                        manchete_min = manchete_limpa.lower()\n",
    "\n",
    "                        try:\n",
    "                            hi_blob = TextBlob(manchete_min)\n",
    "                            manchete_pronta = hi_blob.translate(to='en')\n",
    "                        except:\n",
    "                            manchete_pronta = manchete_min\n",
    "\n",
    "                        lista_manchetes.append(str(manchete_pronta))\n",
    "                        contador += 1\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        indice_letra += 1\n",
    "    loop.close()\n",
    "    #------------------------------------------------------------------\n",
    "    lista_links = []\n",
    "    for linha in data.HTML.index:   #Percorre cada linha do dataset\n",
    "        texto = data.HTML[linha]\n",
    "\n",
    "        contador_de_titulos = 0\n",
    "        for indice in range(len(texto)-7):\n",
    "            if texto[indice:indice+7] == \"message\":\n",
    "                contador_de_titulos += 1\n",
    "\n",
    "\n",
    "\n",
    "        indice = 0\n",
    "        texto_com_url = \"\"\n",
    "\n",
    "        while indice < len(texto):  #Percorre o texto dentro da linha\n",
    "            if texto[indice:indice + 7] == \"widgets\":\n",
    "                while True:\n",
    "                    if texto[indice+5:indice+8] == \"} ]\":\n",
    "                        break\n",
    "                    else:\n",
    "                        texto_com_url += texto[indice+8]\n",
    "                        indice += 1\n",
    "                break \n",
    "            else:\n",
    "                indice += 1\n",
    "\n",
    "\n",
    "        indice = 0\n",
    "        numero_atual_de_titulos = 0\n",
    "\n",
    "        while indice < len(texto_com_url) and numero_atual_de_titulos < contador_de_titulos:\n",
    "            if texto_com_url[indice : indice+3] == \"url\":\n",
    "                link = \"\"\n",
    "                while True:\n",
    "                    if texto_com_url[indice+7] == '\"':\n",
    "                        numero_atual_de_titulos += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        link += texto_com_url[indice+7]\n",
    "                        indice += 1\n",
    "                lista_links.append(link)\n",
    "            else:\n",
    "                indice += 1\n",
    "    #------------------------------------------------------------------\n",
    "    indice_lista = 0\n",
    "    while indice_lista < len(lista_links):\n",
    "        contador_de_https = 0\n",
    "        contador = 0\n",
    "        link = lista_links[indice_lista]\n",
    "        while contador < len(link):\n",
    "            if link[contador:contador+5] == \"https\" or link[contador:contador+4] == \"http\":\n",
    "                contador_de_https += 1\n",
    "            if contador_de_https == 2:\n",
    "                link_completo = link[contador:len(link)]\n",
    "                link_final = \"\"\n",
    "                indice_google = 0\n",
    "                while indice_google < len(link_completo):\n",
    "\n",
    "                    if link_completo[indice_google : indice_google+7] == \"u0026ct\":\n",
    "                        break\n",
    "                    else:\n",
    "                        link_final += link_completo[indice_google]\n",
    "                        indice_google += 1\n",
    "\n",
    "                lista_links[indice_lista] = link_final\n",
    "                break\n",
    "            contador += 1\n",
    "\n",
    "        indice_lista += 1\n",
    "    #------------------------------------------------------------------\n",
    "    lista_backup = []\n",
    "    \n",
    "    loop = tqdm(total = relevantes.shape[0], position = 0, leave = False)\n",
    "    numero_atual = 0\n",
    "    for i in relevantes.index:\n",
    "        loop.set_description(\"Manchetes base OBC...\".format(numero_atual))\n",
    "        loop.update(1)\n",
    "        numero_atual += 1\n",
    "\n",
    "        manchete_relevante = str(relevantes[\"Manchetes da Base de Dados\"][i])\n",
    "        manchete_relevante_limpa = cleanup(manchete_relevante)\n",
    "        manchete_relevante_min = manchete_relevante_limpa.lower()\n",
    "\n",
    "        try:\n",
    "            manchete_relevante_blob = TextBlob(manchete_relevante_min)\n",
    "            manchete_relevante_traduzida = manchete_relevante_blob.translate(to='en')\n",
    "        except:\n",
    "            manchete_relevante_traduzida = manchete_relevante_min\n",
    "\n",
    "        lista_manchetes.append(str(manchete_relevante_traduzida))\n",
    "        lista_backup.append(str(manchete_relevante_traduzida))\n",
    "        lista_links.append(\"Não se aplica\")\n",
    "\n",
    "    loop.close()\n",
    "    \n",
    "    dicionario = {}\n",
    "    \n",
    "    dicionario['Manchetes'] = lista_manchetes\n",
    "    #dicionario['Indice'] = email\n",
    "    dicionario['Link'] = lista_links\n",
    "    \n",
    "    data_manchetes = pd.DataFrame(data=dicionario)\n",
    "    \n",
    "    result = pd.concat([data, data_manchetes], ignore_index=True, axis=1)\n",
    "    \n",
    "    result.columns = ['Data','De','HTML','Resumo','Manchete','Link']\n",
    "    \n",
    "    result[\"Relevância\"] = int(0)\n",
    "    \n",
    "    data_backup = pd.read_excel(\"data.xlsx\", sheet_name=\"backup\")\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    loop = tqdm(total = data_backup.shape[0], position = 0, leave=False)\n",
    "    numero_atual = 0 \n",
    "    for i in data_backup.index:\n",
    "        loop.set_description(\"Adicionando manchetes relevantes...\".format(numero_atual))\n",
    "        loop.update(1)\n",
    "\n",
    "        numero_atual += 1 \n",
    "\n",
    "\n",
    "        manchete_d = str(data_backup.Manchetes[i])\n",
    "        manchete_d_limpa = cleanup(manchete_d)\n",
    "        manchete_d_min = manchete_d_limpa.lower()\n",
    "\n",
    "        try:\n",
    "            manchete_d_min_blob = TextBlob(manchete_d_min)\n",
    "            manchete_d_traduzida = manchete_d_min_blob.translate(to='en')\n",
    "        except:\n",
    "            manchete_d_traduzida = manchete_d_min\n",
    "\n",
    "        lista_backup.append(str(manchete_d_traduzida))\n",
    "\n",
    "    loop.close()\n",
    "    \n",
    "    rel = 0\n",
    "    for i in result.index:\n",
    "        if result[\"Manchete\"][i] in lista_backup:\n",
    "            result.Relevância[i] = int(1)\n",
    "            rel += 1\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando as manchestes do arquivo novo\n",
    "def classifica_novo_data_set(dataset, modelo):\n",
    "    dicionario = dict()\n",
    "    \n",
    "    for linha in dataset.index:\n",
    "        manchete = dataset[\"Manchete\"][linha]\n",
    "        relevancia = int(modelo.predict(manchete))\n",
    "        \n",
    "        if relevancia == 1:\n",
    "            manchetes_relevantes.append(manchete)\n",
    "            links_relevantes.append(dataset[\"Link\"][linha])\n",
    "    \n",
    "    dicionario['Manchetes'] = manchetes_relevantes\n",
    "    #dicionario['Indice'] = email\n",
    "    dicionario['Link'] = links_relevantes\n",
    "    \n",
    "    data_pandas = pd.DataFrame(data=dicionario)\n",
    "    \n",
    "    return data_pandas\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificar (treino, arquivo):\n",
    "    \n",
    "    # Pegando as manchetes e suas respctivas relevâncias \n",
    "    X_para_treinar = treino.loc[:, \"Manchete\"]\n",
    "    Y_para_treinar = treino.loc[:, \"Relevância\"]\n",
    "    ##y = arquivo.loc[:, \"Relevância\"]\n",
    "    \n",
    "    # Vetorizando\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_para_treinar)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    \n",
    "    # Sepando em teste e treino\n",
    "    # Porém vou colocar tudo para treino\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, Y_para_treinar, test_size=0, random_state=42)\n",
    "    \n",
    "    #Aplicando o modelo SVM (Support Vector Machine)\n",
    "\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "\n",
    "    model = svm.SVC(kernel = 'linear') #Kernel\n",
    "\n",
    "    #Dando fit no modelo SVM usando o dataset de treino\n",
    "    model.fit(X_train, y_train)\n",
    " # -------------------------------------------------------------------------------------------------------------   \n",
    "    #Tratando o dataset escolhido\n",
    "    dataset_escolhido_preparado = prepara_novo_data_set(arquivo)\n",
    "    \n",
    "    # Pegando as manchestes do arquivo novo\n",
    "    planilha_pandas = classifica_novo_data_set(dataset_escolhido_preparado, model)\n",
    "    \n",
    "    # Tranformando em Excel\n",
    "    planilha = planilha_pandas.to_excel (r'base_classificada.xlsx ', index = False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
